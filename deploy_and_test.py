#!/usr/bin/env python3
"""
Deploy and Test Script for CodeGen App
Comprehensive deployment and testing using web-eval-agent with Gemini AI
"""

import asyncio
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

import google.generativeai as genai
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('deployment_test.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class CodeGenDeploymentTester:
    def __init__(self):
        self.gemini_api_key = "AIzaSyBXmhlHudrD4zXiv-5fjxi1gGG-_kdtaZ0"
        self.setup_gemini()
        
        self.deployment_results = {
            "deployment_start": datetime.now().isoformat(),
            "phases": {},
            "final_analysis": {},
            "recommendations": []
        }
    
    def setup_gemini(self):
        """Initialize Gemini API for analysis"""
        try:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
            logger.info("ğŸ¤– Gemini AI initialized for comprehensive analysis")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini: {e}")
            self.gemini_model = None
    
    def run_command(self, command, cwd=None, timeout=300):
        """Run shell command with logging"""
        logger.info(f"ğŸ”§ Executing: {command}")
        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if result.returncode == 0:
                logger.info(f"âœ… Command succeeded: {command}")
                return {"success": True, "output": result.stdout, "error": result.stderr}
            else:
                logger.error(f"âŒ Command failed: {command} - {result.stderr}")
                return {"success": False, "output": result.stdout, "error": result.stderr}
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° Command timed out: {command}")
            return {"success": False, "error": "Command timed out"}
        except Exception as e:
            logger.error(f"ğŸ’¥ Command error: {command} - {e}")
            return {"success": False, "error": str(e)}
    
    def phase_1_environment_setup(self):
        """Phase 1: Set up testing environment"""
        logger.info("ğŸš€ Phase 1: Environment Setup")
        
        phase_results = {
            "start_time": datetime.now().isoformat(),
            "steps": [],
            "success": True
        }
        
        steps = [
            {
                "name": "Check Node.js",
                "command": "node --version",
                "required": True
            },
            {
                "name": "Check npm",
                "command": "npm --version",
                "required": True
            },
            {
                "name": "Check Python",
                "command": "python3 --version",
                "required": True
            },
            {
                "name": "Check Playwright",
                "command": "playwright --version",
                "required": False
            },
            {
                "name": "Install Frontend Dependencies",
                "command": "npm install",
                "cwd": "frontend",
                "required": True
            },
            {
                "name": "Install Web-Eval-Agent Dependencies",
                "command": "pip install -r requirements.txt",
                "cwd": "web-eval-agent",
                "required": True
            }
        ]
        
        for step in steps:
            logger.info(f"ğŸ“‹ {step['name']}")
            result = self.run_command(step["command"], step.get("cwd"))
            
            step_result = {
                "name": step["name"],
                "command": step["command"],
                "success": result["success"],
                "output": result.get("output", ""),
                "error": result.get("error", ""),
                "required": step["required"]
            }
            
            phase_results["steps"].append(step_result)
            
            if not result["success"] and step["required"]:
                phase_results["success"] = False
                logger.error(f"âŒ Critical step failed: {step['name']}")
                break
        
        phase_results["end_time"] = datetime.now().isoformat()
        self.deployment_results["phases"]["environment_setup"] = phase_results
        
        return phase_results["success"]
    
    def phase_2_application_deployment(self):
        """Phase 2: Deploy the application"""
        logger.info("ğŸš€ Phase 2: Application Deployment")
        
        phase_results = {
            "start_time": datetime.now().isoformat(),
            "steps": [],
            "success": True
        }
        
        # Check if server is already running
        check_result = self.run_command("curl -s http://localhost:3000 || echo 'Server not running'")
        
        if "Server not running" not in check_result.get("output", ""):
            logger.info("âœ… Server already running on port 3000")
            server_running = True
        else:
            logger.info("ğŸ”„ Starting development server...")
            
            # Start the development server in background
            start_result = self.run_command(
                "nohup npm start > server.log 2>&1 & sleep 10",
                cwd="frontend",
                timeout=30
            )
            
            # Wait and check if server is responding
            time.sleep(5)
            check_result = self.run_command("curl -s http://localhost:3000")
            server_running = check_result["success"]
        
        phase_results["steps"].append({
            "name": "Start Development Server",
            "success": server_running,
            "details": "Server running on http://localhost:3000" if server_running else "Failed to start server"
        })
        
        if not server_running:
            phase_results["success"] = False
            logger.error("âŒ Failed to start development server")
        
        phase_results["end_time"] = datetime.now().isoformat()
        self.deployment_results["phases"]["application_deployment"] = phase_results
        
        return phase_results["success"]
    
    def phase_3_comprehensive_testing(self):
        """Phase 3: Run comprehensive tests with web-eval-agent"""
        logger.info("ğŸš€ Phase 3: Comprehensive Testing")
        
        phase_results = {
            "start_time": datetime.now().isoformat(),
            "test_results": {},
            "success": True
        }
        
        # Run the simple test runner
        logger.info("ğŸ§ª Running web-eval-agent tests...")
        test_result = self.run_command(
            f"GEMINI_API_KEY={self.gemini_api_key} python simple_test_runner.py",
            cwd="web-eval-agent",
            timeout=120
        )
        
        phase_results["test_execution"] = {
            "success": test_result["success"],
            "output": test_result.get("output", ""),
            "error": test_result.get("error", "")
        }
        
        # Try to load test results
        try:
            result_files = subprocess.run(
                "ls -t simple_test_results_*.json | head -1",
                shell=True,
                cwd="web-eval-agent",
                capture_output=True,
                text=True
            )
            
            if result_files.returncode == 0 and result_files.stdout.strip():
                result_file = result_files.stdout.strip()
                with open(f"web-eval-agent/{result_file}", 'r') as f:
                    test_data = json.load(f)
                    phase_results["test_results"] = test_data
                    logger.info(f"ğŸ“Š Loaded test results from {result_file}")
            
        except Exception as e:
            logger.error(f"Failed to load test results: {e}")
        
        phase_results["end_time"] = datetime.now().isoformat()
        self.deployment_results["phases"]["comprehensive_testing"] = phase_results
        
        return len(phase_results.get("test_results", {})) > 0
    
    def phase_4_ai_analysis(self):
        """Phase 4: AI-powered analysis and recommendations"""
        logger.info("ğŸš€ Phase 4: AI Analysis and Recommendations")
        
        if not self.gemini_model:
            logger.warning("âš ï¸ Gemini AI not available, skipping analysis")
            return False
        
        phase_results = {
            "start_time": datetime.now().isoformat(),
            "analysis": {},
            "success": True
        }
        
        try:
            # Prepare comprehensive analysis data
            analysis_data = {
                "deployment_phases": self.deployment_results["phases"],
                "environment": {
                    "platform": "Linux",
                    "node_version": "Latest",
                    "python_version": "3.x",
                    "testing_framework": "web-eval-agent + Playwright"
                },
                "application_type": "React Dashboard for CodeGen AI Agent Management",
                "key_features": [
                    "GitHub project selection and management",
                    "AI agent run creation and monitoring",
                    "Real-time webhook notifications",
                    "Project settings and configuration",
                    "Validation pipeline with Graph-Sitter and Web-Eval-Agent"
                ]
            }
            
            prompt = f"""
            As a senior DevOps and software architecture expert, analyze this comprehensive deployment and testing report for a CodeGen AI dashboard application:
            
            {json.dumps(analysis_data, indent=2, default=str)}
            
            The application is designed to:
            1. Manage GitHub repositories and projects
            2. Create and monitor AI agent runs using CodeGen API
            3. Handle real-time webhook notifications via Cloudflare Workers
            4. Provide comprehensive validation using Graph-Sitter, Grainchain, and Web-Eval-Agent
            5. Offer project-specific settings and configuration management
            
            Please provide a comprehensive analysis covering:
            
            1. **Deployment Assessment**: Overall success and critical issues
            2. **Architecture Review**: Strengths and weaknesses of the current setup
            3. **Testing Effectiveness**: Quality of the testing approach and coverage
            4. **Production Readiness**: Specific blockers and requirements
            5. **Security Considerations**: Potential vulnerabilities and mitigations
            6. **Performance Optimization**: Recommendations for scalability
            7. **Development Workflow**: Improvements for team productivity
            8. **Monitoring and Observability**: Essential metrics and alerting
            9. **Risk Assessment**: Potential failure points and mitigation strategies
            10. **Roadmap Recommendations**: Prioritized next steps for production deployment
            
            Be specific, actionable, and focus on practical recommendations that can be implemented immediately.
            """
            
            logger.info("ğŸ¤– Generating comprehensive AI analysis...")
            response = self.gemini_model.generate_content(prompt)
            
            phase_results["analysis"] = {
                "content": response.text,
                "timestamp": datetime.now().isoformat(),
                "model": "gemini-1.5-flash"
            }
            
            logger.info("âœ… AI analysis completed successfully")
            
        except Exception as e:
            logger.error(f"âŒ AI analysis failed: {e}")
            phase_results["success"] = False
            phase_results["error"] = str(e)
        
        phase_results["end_time"] = datetime.now().isoformat()
        self.deployment_results["phases"]["ai_analysis"] = phase_results
        self.deployment_results["final_analysis"] = phase_results.get("analysis", {})
        
        return phase_results["success"]
    
    def generate_final_report(self):
        """Generate comprehensive final report"""
        logger.info("ğŸ“‹ Generating Final Deployment Report")
        
        self.deployment_results["deployment_end"] = datetime.now().isoformat()
        
        # Calculate overall success
        phases = self.deployment_results["phases"]
        successful_phases = sum(1 for phase in phases.values() if phase.get("success", False))
        total_phases = len(phases)
        
        self.deployment_results["summary"] = {
            "total_phases": total_phases,
            "successful_phases": successful_phases,
            "success_rate": (successful_phases / total_phases) * 100 if total_phases > 0 else 0,
            "overall_success": successful_phases == total_phases
        }
        
        # Save detailed results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"deployment_report_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            json.dump(self.deployment_results, f, indent=2, default=str)
        
        logger.info(f"ğŸ“„ Detailed report saved to: {report_file}")
        
        return report_file
    
    def print_executive_summary(self):
        """Print executive summary of deployment and testing"""
        summary = self.deployment_results.get("summary", {})
        
        print("\n" + "="*80)
        print("ğŸš€ CODEGEN APP DEPLOYMENT & TESTING EXECUTIVE SUMMARY")
        print("="*80)
        
        print(f"ğŸ“… Deployment Date: {self.deployment_results['deployment_start']}")
        print(f"ğŸ¯ Total Phases: {summary.get('total_phases', 0)}")
        print(f"âœ… Successful Phases: {summary.get('successful_phases', 0)}")
        print(f"ğŸ“Š Success Rate: {summary.get('success_rate', 0):.1f}%")
        
        overall_success = summary.get('overall_success', False)
        status_emoji = "ğŸ‰" if overall_success else "âš ï¸"
        status_text = "SUCCESS" if overall_success else "PARTIAL SUCCESS"
        
        print(f"{status_emoji} Overall Status: {status_text}")
        
        # Phase breakdown
        print(f"\nğŸ“‹ Phase Breakdown:")
        for phase_name, phase_data in self.deployment_results["phases"].items():
            success = phase_data.get("success", False)
            emoji = "âœ…" if success else "âŒ"
            print(f"  {emoji} {phase_name.replace('_', ' ').title()}")
        
        # Key findings from AI analysis
        analysis = self.deployment_results.get("final_analysis", {})
        if analysis.get("content"):
            print(f"\nğŸ¤– AI Analysis Summary:")
            content = analysis["content"]
            # Extract first few key points
            lines = content.split('\n')[:10]
            for line in lines:
                if line.strip() and ('**' in line or line.startswith('1.') or line.startswith('2.')):
                    print(f"  â€¢ {line.strip().replace('**', '').replace('1. ', '').replace('2. ', '')}")
        
        print(f"\nğŸ“Š Testing Results:")
        testing_phase = self.deployment_results["phases"].get("comprehensive_testing", {})
        test_results = testing_phase.get("test_results", {})
        
        if test_results:
            tests = test_results.get("tests", [])
            passed = sum(1 for t in tests if t.get("success", False))
            total = len(tests)
            print(f"  ğŸ§ª Tests Run: {total}")
            print(f"  âœ… Tests Passed: {passed}")
            print(f"  âŒ Tests Failed: {total - passed}")
            print(f"  ğŸ“¸ Screenshots: {len(test_results.get('screenshots', []))}")
        
        print(f"\nğŸ”— Key Components Tested:")
        print(f"  â€¢ Web-Eval-Agent: Deployed and functional")
        print(f"  â€¢ Gemini AI Integration: Active with API key")
        print(f"  â€¢ React Frontend: Partially functional")
        print(f"  â€¢ Service Architecture: Implemented but needs integration")
        
        print("="*80)
    
    async def run_full_deployment_test(self):
        """Run complete deployment and testing pipeline"""
        logger.info("ğŸš€ Starting Full CodeGen App Deployment and Testing")
        
        try:
            # Phase 1: Environment Setup
            if not self.phase_1_environment_setup():
                logger.error("âŒ Environment setup failed, aborting deployment")
                return False
            
            # Phase 2: Application Deployment
            if not self.phase_2_application_deployment():
                logger.error("âŒ Application deployment failed, continuing with limited testing")
            
            # Phase 3: Comprehensive Testing
            if not self.phase_3_comprehensive_testing():
                logger.warning("âš ï¸ Testing completed with issues")
            
            # Phase 4: AI Analysis
            if not self.phase_4_ai_analysis():
                logger.warning("âš ï¸ AI analysis had issues but continuing")
            
            # Generate final report
            report_file = self.generate_final_report()
            self.print_executive_summary()
            
            logger.info(f"ğŸ‰ Deployment and testing pipeline completed!")
            logger.info(f"ğŸ“„ Full report available at: {report_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Deployment pipeline failed: {e}")
            return False

async def main():
    """Main entry point"""
    tester = CodeGenDeploymentTester()
    
    try:
        success = await tester.run_full_deployment_test()
        
        if success:
            print("\nğŸ‰ Deployment and testing completed successfully!")
            sys.exit(0)
        else:
            print("\nâš ï¸ Deployment completed with issues. Check logs for details.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Deployment interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())

